
<!-- README.md is generated from README.Rmd. Please edit that file -->

# SVMODT: Support Vector Machine based Oblique Decision Trees

<!-- badges: start -->

[![R-CMD-check](https://github.com/AneeshAgarwala/project-svodt/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/AneeshAgarwala/project-svodt/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

The **svmodt** package in R implements recursive oblique decision trees,
leveraging linear Support Vector Machines (SVMs) to define oblique
splits at each node. While traditional decision trees are valued for
their interpretability due to axis-aligned splits, oblique decision
trees introduce complexity by using linear combinations of features,
making optimal split determination more challenging. SVMs, however,
offer a principled approach to splitting by identifying hyperplanes that
maximize the margin between classes.

## Installation

You can install the development version of svmodt from
[GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("AneeshAgarwala/svmodt")
```

## Key Features

- **Linear SVM splits** for simple decision boundaries

- **Binary classification**; currently only limited to data with binary
  response variable

- **Flexible feature selection** (random, mutual information,
  correlation)

- **Penalized Feature selection** applies to penalty to used features at
  ancestor nodes for diversified feature selection

- **Dynamic Feature Selection** allows user to either randomize or
  decrease the number of features in child nodes

- **Class weight support** for imbalanced data (balanced, balanced
  sub-sample, custom weights)

- **Node-specific scaling** for improved performance

## Examples

``` r
library(svmodt)

# Load data
data(wdbc)  # The package is inclusive of this dataset
wdbc$diagnosis <- factor(wdbc$diagnosis)

# Split
set.seed(123)
train_idx <- sample(nrow(wdbc), 0.8 * nrow(wdbc))
train_data <- wdbc[train_idx, ]
test_data <- wdbc[-train_idx, ]
```

### SVMODT Tree Workflow

``` r
# Train with class weights
tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_depth = 4,
  max_features = 2,
  feature_method = "mutual",
  class_weights = "balanced",
  verbose = TRUE
)

# Predict
predictions <- svm_predict_tree(tree, test_data)

# Visualize - only works for trees with 2 features at each node
viz <- visualize_svm_tree(
  tree = tree,
  original_data = train_data,
  response_col = "diagnosis"
)
```

### Advanced Usage

#### Feature Selection with Penalties

``` r
# Penalize previously used features to promote diversity
tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_depth = 4,
  max_features = 3,
  feature_method = "mutual",
  penalize_used_features = TRUE,
  feature_penalty_weight = 0.5
)
```

#### Dynamic Feature Selection

``` r
set.seed(123)
# Decrease number of features at deeper levels
tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_depth = 5,
  max_features = 10,
  max_features_strategy = "decrease",
  max_features_decrease_rate = 0.8
)

# Random feature selection at each node
tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_features_strategy = "random",
  max_features_random_range = c(0.3, 0.8)
)
```

#### Handle Imbalanced Data

``` r
# Balanced class weights
tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  class_weights = "balanced"
)

set.seed(123)
# Custom class weights
custom_weights <- c("B" = 1, "M" = 3)
tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  class_weights = "custom",
  custom_class_weights = custom_weights
)
```
