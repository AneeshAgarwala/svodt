---
title: An Introduction to Support Vector Machine based Oblique Decision Trees for R
author:
  - name: Aneesh Agarwal
    affiliations: "Master of Business Analytics"
  - name: Jack Jewson
    affiliations: "Supervisor, Department of Econometrics and Business Statistics"
  - name: Erik Sverdrup
    affiliations: "Supervisor, Department of Econometrics and Business Statistics"
date: 20 October 2025
toc: true
format:
  presentation-revealjs+letterbox:
    code-tools: true
    code-block-height: 500px
---

```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  warning = FALSE,
  message = FALSE
)

library(parsnip)
library(rsample)
library(rpart)
library(patchwork)
library(parttree)
library(ggplot2)
library(titanic)
library(dplyr)
library(tidyr)
library(pROC)
library(kernlab)
library(kableExtra)

install.packages("D:/SVMODT/project-svodt/", repos = NULL, type = "source")
devtools::load_all()
```

## Binary Classification Models Overview

::: {style="font-size: 0.65em; overflow-x: auto;"}
| **Model** | **Core Idea** | **Strengths** | **Limitations** |
|:---|:---|:---|:---|
| **Decision Tree** | Splits data using thresholds on features. | Interpretable; handles nonlinearity. | Prone to overfitting; unstable with small changes. |
| **Support Vector Machine (SVM)** | Finds optimal hyperplane maximizing margin. | Effective in high dimensions. | Sensitive to kernel choice; slow for large data. |
| **Neural Network (MLP)** | Learns hierarchical nonlinear relationships. | Handles complex patterns; flexible. | Opaque (“black box”); computationally heavy. |
:::

## How Axis Parallel Decision Trees Split

```{r data}
set.seed(123)
iris_data <- iris |>
  dplyr::filter(Species != "setosa") |>
  dplyr::select(Species, Petal.Length, Sepal.Length)
iris_data$Species <- droplevels(iris_data$Species)
iris_data$Species <- as.factor(iris_data$Species)


split_data <- initial_split(iris_data, prop = 0.8, strata = Species)

train_data <- training(split_data)
test_data <- testing(split_data)

x_train <- train_data[2:3]
y_train <- train_data$Species
x_test <- test_data[2:3]
y_test <- test_data$Species
```

```{r axis-parallel-split-no-depth}
iris_tree <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  fit(Species ~ Sepal.Length + Petal.Length, data = train_data)

pred_rpart_oos <- predict(iris_tree, test_data)
outsample_error <- mean(pred_rpart_oos != as.data.frame(y_test))

pred_rpart_is <- predict(iris_tree, train_data)
insample_error <- mean(pred_rpart_is != as.data.frame(y_train))


train_data %>%
  ggplot(aes(x = Sepal.Length, y = Petal.Length)) +
  geom_jitter(aes(col = Species), alpha = 0.7) +
  geom_parttree(data = iris_tree, aes(fill = Species), alpha = 0.1) +
  theme_minimal() +
  theme(aspect.ratio = 1) +
  coord_flip() +
  labs(
    title = "Axis Parallel Decision Tree Split on Iris Data",
    subtitle = paste(
      "Out-of-sample error: ",
      round(outsample_error * 100, 4), "%",
      "\nIn-sample error",
      round(insample_error * 100, 4),
      "%"
    )
  )
```

## Let's Increase the Tree Depth

```{r axis-parallel-split-added-depth}
iris_tree <- decision_tree(min_n = 1, tree_depth = 3, cost_complexity = 0.02) %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  fit(Species ~ Sepal.Length + Petal.Length, data = train_data)

pred_rpart_oos <- predict(iris_tree, test_data)
outsample_error <- mean(pred_rpart_oos != as.data.frame(y_test))

pred_rpart_is <- predict(iris_tree, train_data)
insample_error <- mean(pred_rpart_is != as.data.frame(y_train))


train_data %>%
  ggplot(aes(y = Sepal.Length, x = Petal.Length)) +
  geom_jitter(aes(col = Species), alpha = 0.7) +
  geom_parttree(data = iris_tree, aes(fill = Species), alpha = 0.1) +
  theme_minimal() +
  theme(aspect.ratio = 1) +
  labs(
    title = "Axis Parallel Decision Tree Split on Iris Data",
    subtitle = paste(
      "Out-of-sample error: ",
      round(outsample_error * 100, 4), "%",
      "\nIn-sample error",
      round(insample_error * 100, 4),
      "%"
    )
  )
```

## Custom Implementation of Decision Trees

```{r}
data <- data.frame(
  Difference = c(
    "Makes pruning decisions at each node during tree growth",
    "Grows the full tree first, then prunes back"
  ),
  Advantages = c(
    "Memory efficient",
    "Considers all pruning possibilities"
  ),
  Limitations = c(
    "Greedy approach",
    "Higher Memory usage"
  )
)

rownames(data) <- c("Custom Tree", "Rpart")

kable(data,
  align = "lccc"
) |>
  kable_styling(
    full_width = FALSE,
    position = "center",
    font_size = 24,
    bootstrap_options = c("striped", "hover", "condensed", "responsive")
  ) |>
  row_spec(0, bold = TRUE, background = "#f0f0f0") |>
  column_spec(1, bold = TRUE, width = "10em") |>
  column_spec(2, width = "18em")
```

```{r echo=TRUE}
#| code-fold: true
base_r_tree <- generate_tree(y_train,
  features = x_train,
  criteria_type = "info_gain",
  # actual depth + 1
  max_depth = 4,
  alpha = 0.0128
)
print_tree(base_r_tree)
```

```{r}
pred_base_r_is <- predict_tree(base_r_tree, x_train)
insample_error <- mean(pred_base_r_is != y_train)

pred_base_r_os <- predict_tree(base_r_tree, x_test)
outsample_error <- mean(pred_base_r_os != y_test)
```

::: {style="font-size: 0.6em; overflow-x: auto;"}
In-sample error: `r round(insample_error, 4)*100` % Out-sample error: `r round(outsample_error, 4)*100` %
:::

## Comparison with R-part Tree

```{r bland-altman-plots-base-r, eval=FALSE}
set.seed(123)

rpart_vec <- c()
base_r_vec <- c()

for (i in 1:100) {
  split_data <- initial_split(wdbc, prop = 0.8, strata = diagnosis)
  train_data <- training(split_data)
  test_data <- testing(split_data)

  x_train <- train_data[, names(wdbc) != "diagnosis"]
  y_train <- train_data$diagnosis
  x_test <- test_data[, names(wdbc) != "diagnosis"]
  y_test <- test_data$diagnosis

  rpart_mod <- rpart(formula = diagnosis ~ ., data = train_data)
  pred_rpart <- predict(rpart_mod, x_test, type = "class")
  rpart_vec[i] <- mean(pred_rpart != y_test)

  tree1 <- generate_tree(y_train, features = x_train, criteria_type = "gini", max_depth = 4, alpha = 0.01)
  p1 <- predict_tree(tree1, x_test)
  base_r_vec[i] <- mean(p1 != y_test)
}

oob_error <- as.data.frame(cbind(rpart_vec, base_r_vec))

p1 <- oob_error |>
  ggplot() +
  geom_point(aes(x = rpart_vec, y = base_r_vec)) +
  geom_abline() +
  labs(y = "Prediction Error (base R)", x = "Prediction Error (rpart)") +
  theme_minimal() +
  theme(aspect.ratio = 1) +
  coord_fixed(ratio = 1)

p2 <- oob_error |>
  ggplot() +
  geom_point(aes(y = base_r_vec - rpart_vec, x = (base_r_vec + rpart_vec) / 2)) +
  geom_hline(yintercept = mean(base_r_vec) - mean(rpart_vec), colour = "blue", linewidth = 1.5) +
  geom_hline(yintercept = mean(base_r_vec) - mean(rpart_vec) + 1.96 * sd(base_r_vec - rpart_vec), colour = "green", linewidth = 1, linetype = 2) +
  geom_hline(yintercept = mean(base_r_vec) - mean(rpart_vec) - 1.96 * sd(base_r_vec - rpart_vec), colour = "green", linewidth = 1, linetype = 2) +
  labs(y = "Difference (Base-R Tree - rpart Tree)", x = "Mean Prediction Error") +
  theme_minimal() +
  theme(aspect.ratio = 1)

p1 + p2 +
  plot_annotation(
    title = "Bland Altman Plots (Base R Decision Trees vs R part)",
    theme = theme(plot.title = element_text(hjust = 0.5))
  )

ggsave(filename = "slide_images/bland-atlman-rpart-vs-base-r.svg")
```

![](slide_images/bland-atlman-rpart-vs-base-r.svg){fig-align="center"}

## 

![](slide_images/paul-allens-split.png){fig-align="center"}

## Oblique Decision Split with SVM

```{r no-depth-svmodt, eval=FALSE}
tree <- svm_split(
  data = train_data,
  response = "Species",
  cost = 2,
  max_depth = 1
)
```

```{r eval=FALSE}
svmodt_pred <- svm_predict_tree(tree = tree, newdata = test_data)
outsample_error <- mean(svmodt_pred != y_test)
pred_svmodt_is <- svm_predict_tree(tree, train_data)
insample_error <- mean(pred_svmodt_is != as.data.frame(y_train))


viz <- visualize_dtsvm_tree(tree = tree, original_data = train_data, features = c("Petal.Length", "Sepal.Length"))

viz$plots$depth_1_ +
  plot_layout(guides = "collect") +
  plot_annotation(subtitle = paste(
    "Out-of-sample error: ",
    round(outsample_error, 4) * 100,
    "%",
    "\nIn-sample error",
    round(insample_error, 4) * 100,
    "%"
  ))

ggsave("slide_images/single-split-svmodt-plot.svg")
```

![](slide_images/single-split-svmodt-plot.svg){fig-align="center"}

## 

![](slide_images/confused-scientist.jpg){fig-align="center"}

## 

![](slide_images/happy-scientist.jpg){fig-align="center" width="550" height="645"}

## Our SVMODT Implementation in Context

::: {style="font-size: 0.5em; overflow-x: auto;"}
| **Feature** | **Our SVMODT Approach** | **Literature Context** | **Practical Advantage** |
|:---|:---|:---|:---|
| **Tree Construction** | Recursive linear SVM at each node. | Similar to Nie (2019) DTSVM. | Fast training; scalable to large datasets. |
| **Split Criterion** | SVM decision values determine splits. | Standard approach across all methods. | Mathematically principled; maximizes margin. |
| **Scaling** | Node-specific scaling at each split. | Novel contribution; not in literature. | Prevents feature scale issues; more robust. |
| **Class Weights** | Balanced or custom weights per node. | Similar to Bala & Agrawal (2010). | Handles imbalanced data effectively. |
| **Feature Selection** | Random/Mutual Info/Correlation with penalties. | Enhanced with penalties (novel). | Promotes feature diversity; reduces overfitting. |
| **Hyperparameters** | depth, min_samples, max_features. | Better than kernel methods. | Easy to tune; less prone to overfitting. |
:::

## SVMODT SPLIT WITH MAX-DEPTH 2

```{r balanced-2-deep-svmodt, eval=TRUE,  echo=TRUE}
#| code-fold: true

tree <- svm_split(
  data = train_data,
  response = "Species",
  cost = 2,
  max_depth = 2,
  class_weights = "balanced",
)
```

```{r eval=FALSE, fig.width=10}
viz <- visualize_dtsvm_tree(tree = tree, original_data = train_data, features = c("Petal.Length", "Sepal.Length"))

svmodt_pred <- svm_predict_tree(tree = tree, newdata = test_data)
outsample_error <- mean(svmodt_pred != y_test)

pred_svmodt_is <- svm_predict_tree(tree, train_data)
insample_error <- mean(pred_svmodt_is != as.data.frame(y_train))


viz$plots$depth_1_ +
  plot_layout(guides = "collect") +
  plot_annotation(
    title = "SVMODT Split on Iris Dataset",
    subtitle = paste(
      "Out-of-sample error: ",
      round(outsample_error, 4) * 100,
      "%",
      "\nIn-sample error",
      round(insample_error, 4) * 100,
      "%"
    )
  )

ggsave("slide_images/depth-unbalanced-split-svmodt-plot.svg")
```

![](slide_images/depth-unbalanced-split-svmodt-plot.svg){fig-align="center"}

## 

```{r eval=FALSE, fig.width= 10}
(viz$plots$`depth_2_Root_→_L` + viz$plots$`depth_2_Root_→_R`) +
  plot_layout(guides = "collect", ) +
  plot_annotation(
    title = "SVMODT Split on Iris Dataset",
    subtitle = paste(
      "Out-of-sample error: ",
      round(outsample_error, 4) * 100,
      "%",
      "\nIn-sample error",
      round(insample_error, 4) * 100,
      "%"
    )
  )

ggsave("slide_images/depth-2-unbalanced-split-svmodt-plot.svg")
```

![](slide_images/depth-2-unbalanced-split-svmodt-plot.svg){fig-align="center" min-scale="2," max-scale="4"}

## How SVMODT Classifies Observations

```{r print-tree-output,  echo=TRUE}
#| code-fold: true
print_svm_tree(tree, show_penalties = FALSE)
```

## Tracing Prediction from SVMODT

```{r trace-prediction-path,  echo=TRUE}
#| code-fold: true

trace_prediction_path(tree = tree, sample_idx = 4, sample_data = test_data)
```

## Wisconsin Breast Cancer Data

::: {style="font-size: 24px; overflow-x: auto;"}
**Response:** Diagnosis (M = malignant, B = benign)

**Features:**

a\) radius (mean of distances from center to points on the perimeter)

b\) texture (standard deviation of gray-scale values)

c\) perimeter

d\) area

e\) smoothness (local variation in radius lengths)

f\) compactness (perimeter^2^ / area - 1.0)

g\) concavity (severity of concave portions of the contour)

h\) concave points (number of concave portions of the contour)

i\) symmetry

j\) fractal dimension ("coastline approximation" - 1)
:::

```{r eval=FALSE}
dim(wdbc) |>
  t() |>
  kableExtra::kable(
    col.names = c("Rows", "Columns"),
    align = "c"
  )
```

## How well does a Decision Tree Perform on WDBC Data?

```{r wdbc-data}
set.seed(123)

split_data <- initial_split(wdbc, prop = 0.8, strata = diagnosis)

train_data <- training(split_data)
test_data <- testing(split_data)

x_train <- train_data[2:31]
y_train <- train_data$diagnosis
x_test <- test_data[2:31]
y_test <- test_data$diagnosis
```

```{r rpart-wdbc-tree, fig.align='center'}
wdbc_rpart_tree <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  fit(diagnosis ~ ., data = train_data)

pred_rpart_oos <- predict(wdbc_rpart_tree, test_data)
outsample_error <- mean(pred_rpart_oos != as.data.frame(y_test))

pred_rpart_is <- predict(wdbc_rpart_tree, train_data)
insample_error <- mean(pred_rpart_is != as.data.frame(y_train))

rattle::fancyRpartPlot(wdbc_rpart_tree$fit,
  main = "Axis Parrallel Splits on WDBC Data",
  sub = paste(
    "Out-of-sample error: ",
    round(outsample_error, 4) * 100,
    "%",
    "\nIn-sample error",
    round(insample_error, 4) * 100,
    "%"
  ),
  caption = ""
)
```

## Fitting SVMODT on WDBC

```{r eval = FALSE, svmodt-feature-selection, echo=TRUE}
#| code-fold: true

tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  cost = 2,
  max_depth = 2,
  max_features = 2,
  feature_method = "mutual", # other option: random(default), cor
  class_weights = "balanced"
)
```

```{r eval=FALSE, fig.width=10}
viz <- visualize_dtsvm_tree(tree = tree, original_data = train_data, check_accuracy = TRUE, response_col = "diagnosis")

svmodt_pred <- svm_predict_tree(tree = tree, newdata = test_data)
outsample_error <- mean(svmodt_pred != y_test)

pred_svmodt_is <- svm_predict_tree(tree, train_data)
insample_error <- mean(pred_svmodt_is != as.data.frame(y_train))

viz$plots$depth_1_ +
  plot_layout(guides = "collect") +
  plot_annotation(
    title = "SVMODT Split on WDBC Dataset",
    subtitle = paste(
      "Out-of-sample error: ",
      round(outsample_error, 4) * 100,
      "%",
      "\nIn-sample error",
      round(insample_error, 4) * 100,
      "%"
    )
  )


ggsave("slide_images/wdbc-balanced-split-svmodt-plot.svg")
```

![](slide_images/wdbc-balanced-split-svmodt-plot.svg){fig-align="center"}

## 

```{r eval=FALSE, fig.width=10}
viz$plots$`depth_2_Root_→_L` + viz$plots$`depth_2_Root_→_R` +
  plot_layout(guides = "collect") +
  plot_annotation(
    title = "SVMODT Split on WDBC Dataset",
    subtitle = paste(
      "Out-of-sample error: ",
      round(outsample_error, 4) * 100,
      "%",
      "\nIn-sample error",
      round(insample_error, 4) * 100,
      "%"
    )
  )

ggsave("slide_images/wdbc-balanced-split-2-svmodt-plot.svg")
```

![](slide_images/wdbc-balanced-split-2-svmodt-plot.svg){fig-align="center"}

## Penalizing Used Features

```{r  echo=TRUE}
#| code-fold: true

tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  cost = 2,
  max_depth = 2,
  max_features = 2,
  feature_method = "mutual", # available options: random(default), cor
  class_weights = "none",
  penalize_used_features = TRUE
)
```

```{r eval=FALSE, fig.width=10}
viz <- visualize_dtsvm_tree(tree = tree, original_data = train_data, check_accuracy = TRUE, response_col = "diagnosis")

svmodt_pred <- svm_predict_tree(tree = tree, newdata = test_data)
outsample_error <- mean(svmodt_pred != y_test)

pred_svmodt_is <- svm_predict_tree(tree, train_data)
insample_error <- mean(pred_svmodt_is != as.data.frame(y_train))

viz$plots$depth_1_ +
  plot_layout(guides = "collect") +
  plot_annotation(
    title = "SVMODT Split on WDBC Dataset",
    subtitle = paste(
      "Out-of-sample error: ",
      round(outsample_error, 4) * 100,
      "%",
      "\nIn-sample error",
      round(insample_error, 4) * 100,
      "%"
    )
  )

ggsave("slide_images/wdbc-penalized-split-svmodt-plot.svg")
```

![](slide_images/wdbc-penalized-split-svmodt-plot.svg){fig-align="center"}

## 

```{r eval=FALSE, fig.width=10}
viz$plots$`depth_2_Root_→_L` + viz$plots$`depth_2_Root_→_R` +
  plot_layout(guides = "collect") +
  plot_annotation(
    title = "SVMODT Split on WDBC Dataset",
    subtitle = paste(
      "Out-of-sample error: ",
      round(outsample_error, 4) * 100,
      "%",
      "\nIn-sample error",
      round(insample_error, 4) * 100,
      "%"
    )
  )

ggsave("slide_images/wdbc-penalized-split-2-svmodt-plot.svg")
```

![](slide_images/wdbc-penalized-split-2-svmodt-plot.svg){fig-align="center"}

## Adding More Features

```{r svmodt-more-features,  echo=FALSE}
tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_depth = 3,
  max_features = 5,
  feature_method = "mutual",
  class_weights = "balanced",
  penalize_used_features = TRUE
)

svmodt_pred <- svm_predict_tree(tree = tree, newdata = test_data)
outsample_error <- mean(svmodt_pred != y_test)

pred_svmodt_is <- svm_predict_tree(tree, train_data)
insample_error <- mean(pred_svmodt_is != as.data.frame(y_train))
```

In-sample error: `r round(insample_error, 4)*100` %

Out-sample error: `r round(outsample_error, 4)*100` %

```{r svmodt-more-features-output,  echo=TRUE}
#| code-fold: true

tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_depth = 3,
  max_features = 5,
  feature_method = "mutual",
  class_weights = "balanced",
  penalize_used_features = TRUE
)

print_svm_tree(tree)
```

## Manipulating Features at Child Node

```{r svmodt-decrease-features}
tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_depth = 3,
  max_features = 5,
  # available options: random, constant (default)
  max_features_strategy = "decrease",
  max_features_decrease_rate = 0.8,
  feature_method = "mutual",
  class_weights = "balanced"
)

svmodt_pred <- svm_predict_tree(tree = tree, newdata = test_data)
outsample_error <- mean(svmodt_pred != y_test)

pred_svmodt_is <- svm_predict_tree(tree, train_data)
insample_error <- mean(pred_svmodt_is != as.data.frame(y_train))
```

In-sample error: `r round(insample_error, 4)*100` %

Out-sample error: `r round(outsample_error, 4)*100` %

```{r svmodt-decrease-features-output, echo=TRUE}
#| code-fold: true

tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_depth = 3,
  max_features = 5,
  # available options: random, constant (default)
  max_features_strategy = "decrease",
  max_features_decrease_rate = 0.8,
  feature_method = "mutual",
  class_weights = "balanced"
)

print_svm_tree(tree)
```

## Adding Random Features

```{r svmodt-random-features}
set.seed(123)
tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_depth = 3,
  feature_method = "mutual",
  max_features_strategy = "random",
  max_features_random_range = c(0.1, 0.3),
  class_weights = "balanced_subsample",
  penalize_used_features = TRUE
)

svmodt_pred <- svm_predict_tree(tree = tree, newdata = test_data)
outsample_error <- mean(svmodt_pred != y_test)

pred_svmodt_is <- svm_predict_tree(tree, train_data)
insample_error <- mean(pred_svmodt_is != as.data.frame(y_train))
```

In-sample error: `r round(insample_error, 4)*100` %

Out-sample error: `r round(outsample_error, 4)*100` %

```{r svmodt-random-features-output,  echo=TRUE}
#| code-fold: true

set.seed(123)
tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_depth = 3,
  feature_method = "mutual",
  max_features_strategy = "random",
  max_features_random_range = c(0.1, 0.3),
  class_weights = "balanced_subsample",
  penalize_used_features = TRUE
)

print_svm_tree(tree)
```

## Model Comparison

```{r results='hide', eval=FALSE, echo=FALSE, fig.width=10}
set.seed(123)

n_iter <- 50
results <- data.frame(
  SVMODT = numeric(n_iter),
  Base_R_Tree = numeric(n_iter),
  RPART = numeric(n_iter),
  RBF_SVM = numeric(n_iter),
  Vanilla_SVM = numeric(n_iter)
)

for (i in 1:n_iter) {
  split_data <- initial_split(wdbc, prop = 0.8, strata = diagnosis)
  train_data <- training(split_data)
  test_data <- testing(split_data)

  x_train <- train_data[, 2:30]
  y_train <- train_data$diagnosis
  x_test <- test_data[, 2:30]
  y_test <- test_data$diagnosis

  #------------------- SVMODT -------------------#
  svmodt_tree <- svm_split(
    data = train_data,
    response = "diagnosis",
    max_depth = 4,
    feature_method = "mutual",
    max_features = 29,
    max_features_strategy = "decrease",
    max_features_decrease_rate = 0.5,
    class_weights = "none",
    penalize_used_features = TRUE
  )

  pred_class <- svm_predict_tree(svmodt_tree, test_data, return_probs = FALSE)
  results$SVMODT[i] <- mean(pred_class == y_test)

  #------------------- Base R Tree -------------------#
  base_r_tree <- generate_tree(y_train,
    features = x_train,
    criteria_type = "info_gain",
    alpha = 0.01
  )
  pred_base_r <- predict_tree(base_r_tree, x_test)
  results$Base_R_Tree[i] <- mean(pred_base_r == y_test)

  #------------------- RPART -------------------#
  rpart_model <- rpart(
    formula = diagnosis ~ ., data = train_data,
    control = rpart.control(cp = 0.05)
  )
  tree_preds <- predict(rpart_model, test_data, type = "class")
  results$RPART[i] <- mean(tree_preds == y_test)

  #------------------- RBF SVM -------------------#
  rbf_model <- ksvm(diagnosis ~ ., data = train_data, kernel = "rbfdot", prob.model = FALSE)
  rbf_pred <- predict(rbf_model, test_data)
  results$RBF_SVM[i] <- mean(rbf_pred == y_test)

  #------------------- Vanilla SVM -------------------#
  vanilla_model <- ksvm(diagnosis ~ ., data = train_data, kernel = "vanilladot", prob.model = FALSE)
  van_pred <- predict(vanilla_model, test_data)
  results$Vanilla_SVM[i] <- mean(van_pred == y_test)
}

#---------------------------------- Summary statistics ----------------------------------#
summary_results <- results %>%
  summarise(across(everything(), list(mean = mean, sd = sd)))

summary_results |>
  t() |>
  kable()

#---------------------------------- Visualization ----------------------------------#

# Boxplot of accuracy distribution
results_long <- tidyr::pivot_longer(results, cols = everything(), names_to = "Model", values_to = "Accuracy")

results_long <- results_long %>%
  mutate(Model = recode(Model,
    "Base_R_Tree" = "Custom Tree (depth=3)",
    "RPART" = "Rpart Tree (cp=0.05)",
    "Vanilla_SVM" = "Linear Kernel SVM",
    "RBF_SVM" = "Radial Basis Kernel SVM",
    "SVMODT" = "SVM-Optimized Decision Tree"
  ))

results_long$Model <- stringr::str_replace_all(results_long$Model, " ", "\n")


ggplot(results_long, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_boxplot(alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Model Accuracy Distribution (50 Resamples)",
    y = "Accuracy", x = ""
  ) +
  theme(
    aspect.ratio = 1,
    legend.position = "none",
    #        axis.text.x = element_text(angle = 45, hjust = 1)
  )
ggsave("slide_images/comparision-boxplot.svg")
```

![](slide_images/comparision-boxplot.svg){fig-align="center"}

```{r comparision-decreasing-features, eval=FALSE, echo=TRUE}
#| code-fold: true

svmodt_tree <- svm_split(
  data = train_data,
  response = "diagnosis",
  max_depth = 4,
  feature_method = "mutual",
  max_features = 29,
  max_features_strategy = "decrease",
  max_features_decrease_rate = 0.5,
  class_weights = "none",
  penalize_used_features = TRUE
)
```

## Bland-Altman Plot

```{r eval=FALSE, fig.width=15}
set.seed(123)
n_iter <- 50
error_results <- data.frame(
  Base_R_Tree = numeric(n_iter),
  RPART = numeric(n_iter),
  SVMODT = numeric(n_iter),
  RBF_SVM = numeric(n_iter)
)

for (i in 1:n_iter) {
  split_data <- initial_split(wdbc, prop = 0.8, strata = diagnosis)
  train_data <- training(split_data)
  test_data <- testing(split_data)

  x_train <- train_data
  y_train <- train_data$diagnosis
  x_test <- test_data
  y_test <- test_data$diagnosis

  #------------------- SVMODT -------------------#
  svmodt_tree <- svm_split(
    data = train_data,
    response = "diagnosis",
    max_depth = 4,
    feature_method = "mutual",
    max_features = 29,
    max_features_strategy = "decrease",
    max_features_decrease_rate = 0.5,
    class_weights = "none",
    penalize_used_features = TRUE
  )

  pred_class <- svm_predict_tree(svmodt_tree, test_data, return_probs = FALSE)
  error_results$SVMODT[i] <- mean(pred_class != y_test)

  #------------------- RBF SVM -------------------#
  rbf_model <- ksvm(diagnosis ~ ., data = train_data, kernel = "rbfdot", prob.model = TRUE)
  rbf_pred <- predict(rbf_model, test_data)
  error_results$RBF_SVM[i] <- mean(rbf_pred != y_test)
}

## RBF vs SVMODT
diff_svm <- error_results$RBF_SVM - error_results$SVMODT
mean_svm <- (error_results$RBF_SVM + error_results$SVMODT) / 2

p1 <- error_results |>
  ggplot() +
  geom_point(aes(x = RBF_SVM, y = SVMODT)) +
  geom_abline() +
  labs(y = "SVMODT", x = "RBF SVM") +
  theme_minimal() +
  coord_fixed(ratio = 1)

p2 <- ggplot(data.frame(mean_svm, diff_svm), aes(x = mean_svm, y = diff_svm)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = mean(diff_svm), colour = "blue", size = 1.2) +
  geom_hline(yintercept = mean(diff_svm) + 1.96 * sd(diff_svm), linetype = "dashed", colour = "red") +
  geom_hline(yintercept = mean(diff_svm) - 1.96 * sd(diff_svm), linetype = "dashed", colour = "red") +
  labs(
    x = "Mean Prediction Error",
    y = "Difference in Error (RBF – SVMODT)"
  ) +
  theme_minimal() +
  theme(aspect.ratio = 1)

# Show both plots side by side
p1 + p2 +
  plot_annotation(
    title = "Bland–Altman: RBF SVM vs SVMODT",
    theme = theme(plot.title = element_text(hjust = 0.5))
  )

ggsave(filename = "slide_images/bland-atlman-svmodt-vs-rbfsvm.svg")
```

![](slide_images/bland-atlman-svmodt-vs-rbfsvm.svg){fig-align="center" width="674"}

## SVMODT Performance on MIMIC-III Data

::: {style="font-size:30px"}
Deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. <br> **Response:** Death of a patient that is entering an ICU <br>

**Data Cleaning/Feature Engineering**

-   Calculated **Charlson Comorbidity Index**

-   Counted **additional diagnoses per admission**

-   Kept only numeric predictors for model training
:::

<br>

::: {#Results style="font-size: 0.65em; overflow-x: auto;"}
| Model   | SVMODT | RBF SVM | Linear SVM | Decision Tree |
|---------|--------|---------|------------|---------------|
| **AUC** | 0.790  | 0.748   | 0.713      | 0.767         |
:::

## Current Limitations

::::: columns
::: {.column width="50%" style="font-size:0.8em;"}
**1. Hyperparameter Complexity**

-   Multiple tuning parameters (*max_depth*, *min_samples*, *max_features*, *penalty_weight*)

-   Feature selection strategy requires careful consideration

**2. Binary Classification Focus**

-   Currently designed for two-class problems
:::

::: {.column width="50%" style="font-size:0.8em;"}
**3. Interpretability Trade-off**

-   SVM decision boundaries less intuitive than pure thresholds

-   Feature interactions harder to explain

**4. Computational Considerations**

-   Node-specific scaling adds overhead

-   Feature penalty calculations at each split

-   Memory requirements for storing multiple SVM models
:::
:::::

## Future Directions

::: {style="font-size:0.9em;"}
-   Automated hyperparameter tuning (Bayesian/meta-learning)

-   Native multi-class node splits

-   SHAP/LIME-based explainability

-   Parallel & approximate SVM scalability

-   Ensemble variants with random feature subsets
:::

## 

::: {style="display: flex; flex-direction: column; justify-content: center; align-items: center; height: 80vh; text-align: center;"}
<h2 style="font-size: 2.5em; margin-bottom: 0.3em;">

Thank You!

</h2>

<h3 style="font-size: 1.8em; margin-bottom: 0.5em;">

Questions?

</h3>
:::
