% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/svmodt_tree.R
\name{svm_split}
\alias{svm_split}
\title{Build an Oblique Decision Tree Using SVM Splits}
\usage{
svm_split(
  data,
  response,
  depth = 1,
  max_depth = 3,
  min_samples = 5,
  max_features = NULL,
  feature_method = c("random", "mutual", "cor"),
  max_features_strategy = c("constant", "random", "decrease"),
  max_features_decrease_rate = 0.8,
  max_features_random_range = c(0.3, 1),
  penalize_used_features = FALSE,
  feature_penalty_weight = 0.5,
  used_features = character(0),
  class_weights = c("none", "balanced", "balanced_subsample", "custom"),
  custom_class_weights = NULL,
  verbose = FALSE,
  all_classes = NULL,
  ...
)
}
\arguments{
\item{data}{A data frame containing predictors and the response variable.}

\item{response}{Character string specifying the response column in `data`.
All other columns are treated as predictors.}

\item{depth}{Integer indicating the current recursion depth (used internally; default is 1).}

\item{max_depth}{Maximum depth of the tree.}

\item{min_samples}{Minimum number of samples required to attempt a split.}

\item{max_features}{Maximum number of features to consider at each split.}

\item{feature_method}{Feature selection method at each node. One of:
\itemize{
  \item `"random"`: randomly select features,
  \item `"mutual"`: select based on mutual information with the response,
  \item `"cor"`: select based on correlation with the response.
}}

\item{max_features_strategy}{Strategy to adjust the number of features per node:
\itemize{
  \item `"constant"`: keep `max_features` constant,
  \item `"decrease"`: reduce features with depth,
  \item `"random"`: randomly vary number of features within a range.
}}

\item{max_features_decrease_rate}{Numeric fraction for decreasing features if
`max_features_strategy = "decrease"`.}

\item{max_features_random_range}{Numeric vector of length 2 specifying min and max
fraction of features if `max_features_strategy = "random"`.}

\item{penalize_used_features}{Logical; if TRUE, features used in ancestor nodes
are penalized to encourage diversity.}

\item{feature_penalty_weight}{Numeric (0–1) weight for penalizing previously used features.}

\item{used_features}{Character vector of features already used in ancestor nodes
(used internally).}

\item{class_weights}{Character string specifying how to handle class imbalance. One of:
\itemize{
  \item `"none"`: no weighting,
  \item `"balanced"`: weight classes inversely proportional to their frequency,
  \item `"balanced_subsample"`: weight per node based on local class distribution,
  \item `"custom"`: use `custom_class_weights`.
}}

\item{custom_class_weights}{Optional named numeric vector specifying custom weights per class.}

\item{verbose}{Logical; if TRUE, prints information about each node during tree construction.}

\item{all_classes}{Optional character vector of all possible response classes (used internally).}

\item{...}{Additional arguments passed to the underlying SVM fitting function.}
}
\value{
A nested list representing the decision tree. Each node contains:
\describe{
  \item{is_leaf}{Logical; TRUE if the node is a leaf.}
  \item{model}{Fitted SVM model at this node (for internal nodes).}
  \item{features}{Vector of features selected for this node.}
  \item{scaler}{Scaling information used at this node.}
  \item{left}{Left child node (decision value > 0).}
  \item{right}{Right child node (decision value ≤ 0).}
  \item{depth}{Depth of this node in the tree.}
  \item{n}{Number of samples at this node.}
  \item{max_features_used}{Number of features considered at this node.}
  \item{penalty_applied}{Logical; TRUE if feature penalization was applied.}
  \item{class_weights_used}{Class weights applied at this node.}
}
}
\description{
Constructs a decision tree where each internal node uses a Support Vector
Machine (SVM) to determine the split. Supports dynamic feature selection,
feature penalization, scaling, and class weighting.
}
\details{
This function recursively splits the dataset using an SVM at each node. Splitting
stops when maximum depth is reached, the node contains fewer than `min_samples`,
or all samples belong to the same class. Features are scaled and selected dynamically
at each node, and previously used features can be penalized to promote diversity.
Class weighting schemes support handling imbalanced datasets. This approach allows
construction of an **oblique decision tree**, where splits are linear hyperplanes
rather than axis-aligned.
}
\examples{
data(wdbc)
tree <- svm_split(
  data = wdbc,
  response = "diagnosis",
  max_depth = 3,
  min_samples = 5,
  feature_method = "random",
  verbose = TRUE
)

}
