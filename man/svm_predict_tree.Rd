% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/svmodt_predict_tree.R
\name{svm_predict_tree}
\alias{svm_predict_tree}
\title{Predict Using a Support Vector Machine Oblique Decision Tree}
\usage{
svm_predict_tree(tree, newdata, return_probs = FALSE, calibrate_probs = TRUE)
}
\arguments{
\item{tree}{A tree node object (leaf or internal) created by \code{svm_split} or \code{svm_split_enhanced}.}

\item{newdata}{A data frame of new predictor values. **Must contain the same features** as those used to fit the tree.
Any additional columns (including responses) are ignored.}

\item{return_probs}{Logical; if \code{TRUE}, returns both predicted class labels and class probabilities.}

\item{calibrate_probs}{Logical; if \code{TRUE}, converts SVM decision values to probabilities using logistic
calibration (sigmoid) based on the distance from the hyperplane. If \code{FALSE}, fallback probabilities
are computed from class frequencies at the leaf node.}
}
\value{
If \code{return_probs = FALSE}, a character vector of predicted class labels.
If \code{return_probs = TRUE}, a list with elements:
\itemize{
  \item \code{predictions}: Character vector of predicted class labels.
  \item \code{probabilities}: Numeric matrix of class probabilities
    (rows = samples, columns = classes).
}
}
\description{
Predicts class labels or class probabilities for new data using a tree
constructed with SVM splits. Handles leaf nodes, internal nodes, recursive traversal,
and fallback mechanisms when SVM predictions or scaling fail.
}
\details{
The function traverses the SVM-based oblique decision tree recursively and predicts class labels or probabilities. Key behaviors:
\itemize{
  \item \strong{Leaf nodes:} Return the majority class stored in the node, along with class probabilities.
  \item \strong{Internal nodes:}
    \itemize{
      \item Scale features according to the nodeâ€™s scaling parameters.
      \item Compute SVM decision values.
      \item Recursively traverse left and right children depending on the sign of the decision value.
    }
  \item \strong{Binary support:}
    \itemize{
      \item Binary SVMs produce a single decision value per node.
    }
  \item \strong{Fallback predictions:} If scaling fails, SVM predictions are unavailable, or child nodes are missing, predictions are generated in this order:
    \itemize{
      \item SVM-provided probabilities (if available).
      \item Calibrated decision values using a logistic/sigmoid function (if \code{calibrate_probs = TRUE}).
      \item Leaf node class distribution (empirical frequencies) or uniform probabilities as a last resort.
    }
  \item \strong{Probability normalization:} All returned probabilities are normalized so that each row sums to 1.
  \item \strong{Feature requirement:} \code{newdata} must contain exactly the features used to train the tree; any extra columns, including responses, are ignored.
  \item \strong{Calibration behavior:}
    \itemize{
      \item \code{calibrate_probs = FALSE} returns class frequencies at the leaf node.
      \item \code{calibrate_probs = TRUE} uses the distance from the hyperplane for logistic post-processing into probabilities.
    }
}
}
\examples{
\dontrun{
# Train DTSVM tree
tree <- svm_split_enhanced(
  data = wdbc,
  response = "diagnosis",
  max_depth = 3,
  max_features = 2,
  feature_method = "cor",
  class_weights = "balanced_subsample"
)

# Predict on WDBC data
preds <- predict_svm_tree(tree, newdata = wdbc)

# Predict with probabilities and logistic calibration
result <- predict_svm_tree(tree, newdata = wdbc,
                           return_probs = TRUE, calibrate_probs = TRUE)
}
}
