% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/custom_generate_tree.R
\name{generate_tree}
\alias{generate_tree}
\title{Build a Decision Tree with Cost-Complexity Pruning}
\usage{
generate_tree(
  target,
  features,
  criteria_type = c("gini", "info_gain", "gain_ratio"),
  ig_metric = c("gini", "entropy"),
  depth = 1,
  max_depth = 10,
  all_levels = NULL,
  alpha = 0
)
}
\arguments{
\item{target}{A vector of class labels (factor or character) for the samples.}

\item{features}{A data frame or matrix of predictor variables.}

\item{criteria_type}{Splitting criterion: \code{"gini"}, \code{"info_gain"}, or \code{"gain_ratio"}.}

\item{ig_metric}{Impurity metric to use for \code{info_gain}: \code{"gini"} or \code{"entropy"}.}

\item{depth}{Internal use: current recursion depth (default = 1).}

\item{max_depth}{Maximum allowed depth of the tree (default = 10).}

\item{all_levels}{Optional vector of all possible class levels. If \code{NULL}, inferred from \code{target}.}

\item{alpha}{Cost-complexity pruning parameter. Higher values increase pruning (default = 0).}
}
\value{
A nested list representing the decision tree. Each node contains:
\itemize{
  \item \code{split_feature}: Name of the feature used for splitting (internal nodes only).
  \item \code{split_value}: Numeric threshold or factor level used for splitting (internal nodes only).
  \item \code{left}: Left child subtree (samples satisfying the split condition).
  \item \code{right}: Right child subtree.
  \item \code{probs}: Named vector of class probabilities at the node.
  \item \code{prediction}: Majority class at the node.
  \item \code{n}: Number of samples at the node.
}
}
\description{
Constructs a decision tree for classification using a chosen splitting criterion
(Gini impurity, information gain, or gain ratio). Supports numeric and factor
features and integrates cost–complexity pruning via an \code{alpha} parameter.
}
\details{
The function recursively grows a classification tree:
\itemize{
  \item Stops if maximum depth is reached, or if the node is pure (all samples belong to one class).
  \item Selects the best feature and split point using the chosen \code{criteria_type}.
  \item Handles numeric and factor features automatically.
  \item Implements cost–complexity pruning: compares subtree error with leaf error, scaled by \code{alpha}.
  \item Returns a leaf node if no valid split exists or if pruning criterion favors collapsing the node.
}
}
\examples{
# Example with wdbc dataset
data(wdbc)
features <- wdbc[, 2:31]
target <- wdbc$diagnosis
tree <- generate_tree(target, features, criteria_type = "gini", max_depth = 3, alpha = 0.01)

# Access root node prediction
tree$prediction

# Access left child subtree
tree$left

}
